{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4753077,"sourceType":"datasetVersion","datasetId":2750746}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing libraries, loading and transforming data","metadata":{}},{"cell_type":"code","source":"# Install the 'evaluate' library with the specified version (4.28.1) quietly (-q).\n!pip install -q evaluate transformers==4.28.1\n\n# Upgrade the 'datasets' library to the latest version quietly (-q).\n!pip install -U -q datasets\n\n# Install the 'torchaudio' library with the specified version (0.12.0+cu113) from the provided CUDA version repository.\n!pip install -q torchaudio==0.12.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n\n# Add the 'ffmpeg4' repository to the package manager's sources list (-y for yes).\n!add-apt-repository -y ppa:savoury1/ffmpeg4 \n\n# Install the 'ffmpeg' package quietly (-qq).\n!apt-get -qq install -y ffmpeg\n\n# Install the 'mlflow' library quietly (-q).\n!pip install -q mlflow","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:13:44.157413Z","iopub.execute_input":"2023-12-17T12:13:44.157754Z","iopub.status.idle":"2023-12-17T12:18:03.543698Z","shell.execute_reply.started":"2023-12-17T12:13:44.157726Z","shell.execute_reply":"2023-12-17T12:18:03.542276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd  # Pandas for data manipulation\nimport gc  # Garbage collection module\nimport re  # Regular expressions for text processing\nimport numpy as np  # NumPy for numerical operations\n\n# Suppress warnings\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n# Import tqdm for progress tracking\nfrom tqdm import tqdm\ntqdm.pandas()\n\n# Import Path from pathlib for working with file paths\nfrom pathlib import Path\n\n# Import oversampling and undersampling methods from imblearn\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Import class_weight calculation function from scikit-learn\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Import matplotlib for data visualization\nimport matplotlib.pyplot as plt\n\n# Import itertools for working with iterators\nimport itertools\n\n# Import various metrics from scikit-learn\nfrom sklearn.metrics import (\n    accuracy_score,  # For calculating accuracy\n    roc_auc_score,   # For ROC AUC score\n    confusion_matrix,  # For confusion matrix\n    classification_report,  # For classification report\n    f1_score  # For F1 score\n)\n\n# Import PyTorch for deep learning\nimport torch\n\n# Import the Hugging Face Transformers library\nimport transformers\n\n# Print the version of the transformers library\nprint(transformers.__version__)\n\n# Import torchaudio for audio processing with PyTorch\nimport torchaudio\n\n# Print the version of torchaudio\nprint(torchaudio.__version__)\n\n# Import a custom module named 'evaluate' for evaluation functions\nimport evaluate\n\n# Import Audio for displaying audio clips in the notebook\nfrom IPython.display import Audio\n\n# Import various classes and modules from Hugging Face Transformers and Datasets\nfrom transformers import AutoFeatureExtractor, AutoModelForAudioClassification, pipeline, TrainingArguments, Trainer\nfrom datasets import Dataset, Image, ClassLabel  # Import custom 'Dataset', 'ClassLabel', and 'Image' classes","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:18:03.546546Z","iopub.execute_input":"2023-12-17T12:18:03.547314Z","iopub.status.idle":"2023-12-17T12:18:17.801108Z","shell.execute_reply.started":"2023-12-17T12:18:03.547271Z","shell.execute_reply":"2023-12-17T12:18:17.80014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the resampling rate in Hertz (Hz) for audio data\nRATE_HZ = 16000\n\n# Define the maximum audio interval length to consider in seconds\nMAX_SECONDS = 10\n\n# Calculate the maximum audio interval length in samples by multiplying the rate and seconds\nMAX_LENGTH = RATE_HZ * MAX_SECONDS\n\n# Define the minimum number of records per label required for the dataset\nMIN_RECORDS_PER_LABEL = 25\n\n# Define the fraction of records to be used for testing data\nTEST_SIZE = 0.1\n\n# Ensure that the product of MIN_RECORDS_PER_LABEL and TEST_SIZE is greater than 2\n# This ensures a sufficient number of samples for testing","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:18:17.802256Z","iopub.execute_input":"2023-12-17T12:18:17.802556Z","iopub.status.idle":"2023-12-17T12:18:17.807788Z","shell.execute_reply.started":"2023-12-17T12:18:17.802531Z","shell.execute_reply":"2023-12-17T12:18:17.806846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to load bird sound data from a specified directory.\ndef load_data():\n    # Initialize empty lists to store file paths and corresponding labels.\n    file_list = []  # To store file paths\n    label_list = []  # To store labels\n\n    # Iterate through all the .mp3 files in the specified directory and its subdirectories.\n    for file in Path('/kaggle/input/sound-of-114-species-of-birds-till-2022/Voice of Birds/Voice of Birds/').glob('*/*.mp3'):\n        # Extract the label from the file path by splitting the path and retrieving the second-to-last part.\n        # The label is assumed to be the second-to-last part, separated by '/' and '_' characters.\n        label = str(file).split('/')[-2].split('_')[0]\n\n        # Append the current file path to the file_list and its corresponding label to the label_list.\n        file_list.append(file)\n        label_list.append(label)\n\n    # Create an empty DataFrame to organize the data.\n    dd = pd.DataFrame()\n\n    # Create two columns in the DataFrame: 'file' to store file paths and 'label' to store labels.\n    dd['file'] = file_list\n    dd['label'] = label_list\n\n    # Return the DataFrame containing the file paths and labels.\n    return dd","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:18:17.810054Z","iopub.execute_input":"2023-12-17T12:18:17.810337Z","iopub.status.idle":"2023-12-17T12:18:17.861406Z","shell.execute_reply.started":"2023-12-17T12:18:17.810312Z","shell.execute_reply":"2023-12-17T12:18:17.860427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Load the data into a DataFrame\ndf = load_data()  # Assuming there's a function called load_data() that loads data into 'df'.\n\n# Sample 5 random rows from the DataFrame\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:18:17.862644Z","iopub.execute_input":"2023-12-17T12:18:17.862992Z","iopub.status.idle":"2023-12-17T12:18:18.472856Z","shell.execute_reply.started":"2023-12-17T12:18:17.862965Z","shell.execute_reply":"2023-12-17T12:18:18.47192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate label counts\nlabel_counts = df['label'].value_counts()\n\n# Identify undersampled labels\nundersampled_labels = label_counts[label_counts < MIN_RECORDS_PER_LABEL].index\n\n# Remove rows with undersampled labels\ndf = df[~df['label'].isin(undersampled_labels)]\n\n# Print the shape of the resulting DataFrame\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:18:18.474059Z","iopub.execute_input":"2023-12-17T12:18:18.474331Z","iopub.status.idle":"2023-12-17T12:18:18.486058Z","shell.execute_reply.started":"2023-12-17T12:18:18.474307Z","shell.execute_reply":"2023-12-17T12:18:18.48509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrieve unique values in the 'label' column of the DataFrame 'df'\nunique_labels = df['label'].unique()\nunique_labels","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:18:18.487321Z","iopub.execute_input":"2023-12-17T12:18:18.48765Z","iopub.status.idle":"2023-12-17T12:18:18.496741Z","shell.execute_reply.started":"2023-12-17T12:18:18.487613Z","shell.execute_reply":"2023-12-17T12:18:18.495581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # This function takes a file path as input and performs several audio transformations.\n# def get_transform_audio(file):\n#     try:\n#         # Load the audio file using torchaudio and get its sample rate.\n#         audio, rate = torchaudio.load(str(file))\n        \n#         # Create a transformation to resample the audio to a specified sample rate (RATE_HZ).\n#         transform = torchaudio.transforms.Resample(rate, RATE_HZ)\n        \n#         # Apply the resampling transformation to the audio and convert it to a NumPy array.\n#         audio = transform(audio).squeeze(0).numpy().reshape(-1)\n        \n#         # Truncate the audio to the first MAX_LENGTH samples to save memory.\n#         audio = audio[:MAX_LENGTH]\n        \n#         # Return the preprocessed audio data.\n#         return audio\n#     except:\n#         # If an exception occurs (e.g., file not found), return None.\n#         return None\n\n# # Apply the 'get_transform_audio' function to each file path in the 'df' DataFrame\n# # and store the preprocessed audio in a new 'audio' column.\n# df['audio'] = df['file'].progress_apply(get_transform_audio)\n\n# Split files by chunks with == MAX_LENGTH size\ndef split_audio(file):\n    try:\n        # Load the audio file using torchaudio and get its sample rate.\n        audio, rate = torchaudio.load(str(file))\n\n        # Calculate the number of segments based on the MAX_LENGTH\n        num_segments = (len(audio[0]) // MAX_LENGTH)  # Floor division to get segments\n\n        # Create an empty list to store segmented audio data\n        segmented_audio = []\n\n        # Split the audio into segments\n        for i in range(num_segments):\n            start = i * MAX_LENGTH\n            end = min((i + 1) * MAX_LENGTH, len(audio[0]))\n            segment = audio[0][start:end]\n\n            # Create a transformation to resample the audio to a specified sample rate (RATE_HZ).\n            transform = torchaudio.transforms.Resample(rate, RATE_HZ)\n            segment = transform(segment).squeeze(0).numpy().reshape(-1)\n\n            segmented_audio.append(segment)\n\n        # Create a DataFrame from the segmented audio\n        df_segments = pd.DataFrame({'audio': segmented_audio})\n\n        return df_segments\n\n    except Exception as e:\n        # If an exception occurs (e.g., file not found), return nothing\n        print(f\"Error processing file: {e}\")\n        return None\n    \ndf_list = []\nfor input_file, input_label in tqdm(zip(df['file'].values, df['label'].values)):\n    resulting_df = split_audio(input_file)\n    if resulting_df is not None:\n        resulting_df['label'] = input_label\n        df_list.append(resulting_df)\ndf = pd.concat(df_list, axis=0)\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:18:18.498094Z","iopub.execute_input":"2023-12-17T12:18:18.498392Z","iopub.status.idle":"2023-12-17T12:27:28.883655Z","shell.execute_reply.started":"2023-12-17T12:18:18.498366Z","shell.execute_reply":"2023-12-17T12:27:28.88266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_list\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:27:28.884998Z","iopub.execute_input":"2023-12-17T12:27:28.885291Z","iopub.status.idle":"2023-12-17T12:27:29.224457Z","shell.execute_reply.started":"2023-12-17T12:27:28.885265Z","shell.execute_reply":"2023-12-17T12:27:29.223484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting rows in the DataFrame where the 'audio' column is not null (contains non-missing values).\ndf = df[~df['audio'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:27:29.229113Z","iopub.execute_input":"2023-12-17T12:27:29.22944Z","iopub.status.idle":"2023-12-17T12:27:29.240763Z","shell.execute_reply.started":"2023-12-17T12:27:29.22941Z","shell.execute_reply":"2023-12-17T12:27:29.239833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:27:29.241969Z","iopub.execute_input":"2023-12-17T12:27:29.242308Z","iopub.status.idle":"2023-12-17T12:27:29.264548Z","shell.execute_reply.started":"2023-12-17T12:27:29.242281Z","shell.execute_reply":"2023-12-17T12:27:29.263474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing the 'file' column from the DataFrame 'df'\nif 'file' in df.columns:\n    df = df.drop(['file'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:27:29.268026Z","iopub.execute_input":"2023-12-17T12:27:29.268375Z","iopub.status.idle":"2023-12-17T12:27:29.273363Z","shell.execute_reply.started":"2023-12-17T12:27:29.268347Z","shell.execute_reply":"2023-12-17T12:27:29.272409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataset from the Pandas DataFrame 'df'\ndataset = Dataset.from_pandas(df)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:27:29.274635Z","iopub.execute_input":"2023-12-17T12:27:29.274992Z","iopub.status.idle":"2023-12-17T12:27:57.91027Z","shell.execute_reply.started":"2023-12-17T12:27:29.274962Z","shell.execute_reply":"2023-12-17T12:27:57.90926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify the unique classes in the training data.\nclasses = np.unique(df[['label']])\n\nprint(classes)\n\n# Calculate class weights using the 'balanced' option, which automatically adjusts for class imbalance.\nweights = compute_class_weight(class_weight='balanced', classes=classes, y=df['label'])\n\n# Create a dictionary mapping each class to its respective class weight.\nclass_weights = dict(zip(classes, weights))\n\n# Print the computed class weights to the console.\nprint(class_weights)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:27:57.911525Z","iopub.execute_input":"2023-12-17T12:27:57.911798Z","iopub.status.idle":"2023-12-17T12:27:57.957216Z","shell.execute_reply.started":"2023-12-17T12:27:57.911773Z","shell.execute_reply":"2023-12-17T12:27:57.956334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a list of unique labels\nlabels_list = sorted(list(df['label'].unique()))\n\n# Initialize empty dictionaries to map labels to IDs and vice versa\nlabel2id, id2label = dict(), dict()\n\n# Iterate over the unique labels and assign each label an ID, and vice versa\nfor i, label in enumerate(labels_list):\n    label2id[label] = i  # Map the label to its corresponding ID\n    id2label[i] = label  # Map the ID to its corresponding label\n\n# Print the resulting dictionaries for reference\nprint(\"Mapping of IDs to Labels:\", id2label, '\\n')\nprint(\"Mapping of Labels to IDs:\", label2id)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:27:57.958311Z","iopub.execute_input":"2023-12-17T12:27:57.958664Z","iopub.status.idle":"2023-12-17T12:27:57.966815Z","shell.execute_reply.started":"2023-12-17T12:27:57.958637Z","shell.execute_reply":"2023-12-17T12:27:57.96581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating classlabels to match labels to IDs\nClassLabels = ClassLabel(num_classes=len(labels_list), names=labels_list)\n\n# Mapping labels to IDs\ndef map_label2id(example):\n    example['label'] = ClassLabels.str2int(example['label'])\n    return example\n\ndataset = dataset.map(map_label2id, batched=True)\n\n# Casting label column to ClassLabel Object\ndataset = dataset.cast_column('label', ClassLabels)\n\n# Splitting the dataset into training and testing sets using the predefined train/test split ratio.\ndataset = dataset.train_test_split(test_size=TEST_SIZE, shuffle=True, stratify_by_column=\"label\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:27:57.968113Z","iopub.execute_input":"2023-12-17T12:27:57.968403Z","iopub.status.idle":"2023-12-17T12:28:21.571052Z","shell.execute_reply.started":"2023-12-17T12:27:57.968378Z","shell.execute_reply":"2023-12-17T12:28:21.570192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deleting the DataFrame 'df'\ndel df\n\n# Performing garbage collection to free up memory\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:28:21.572528Z","iopub.execute_input":"2023-12-17T12:28:21.573143Z","iopub.status.idle":"2023-12-17T12:28:21.926183Z","shell.execute_reply.started":"2023-12-17T12:28:21.573103Z","shell.execute_reply":"2023-12-17T12:28:21.925209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load facebook/wav2vec2-base-960h model","metadata":{}},{"cell_type":"code","source":"# Specify the pre-trained model you want to use.\nmodel_str = \"dima806/bird_sounds_classification\" #\"facebook/wav2vec2-base-960h\"\n\n# Create an instance of the feature extractor for audio.\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_str)\n\n# Create an instance of the audio classification model.\n# The 'num_labels' parameter is set to the number of labels in your 'labels_list'.\nmodel = AutoModelForAudioClassification.from_pretrained(model_str, num_labels=len(labels_list))\n\n# Set the 'id2label' mapping in the model's configuration. This maps label IDs to human-readable labels.\nmodel.config.id2label = id2label\n\n# Calculate and print the number of trainable parameters in the model (in millions).\n# This provides an estimate of the model's size.\nprint(model.num_parameters(only_trainable=True) / 1e6)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:28:21.928116Z","iopub.execute_input":"2023-12-17T12:28:21.928508Z","iopub.status.idle":"2023-12-17T12:28:39.388609Z","shell.execute_reply.started":"2023-12-17T12:28:21.928471Z","shell.execute_reply":"2023-12-17T12:28:39.387633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a preprocessing function for the dataset\ndef preprocess_function(batch):\n    # Extract audio features from the input batch using the feature_extractor\n    inputs = feature_extractor(batch['audio'], sampling_rate=RATE_HZ, max_length=MAX_LENGTH, truncation=True)\n    \n    # Extract and store only the 'input_values' component from the extracted features\n    inputs['input_values'] = inputs['input_values'][0]\n    \n    return inputs\n\n# Apply the preprocess_function to the 'train' split of the dataset, removing the 'audio' column\ndataset['train'] = dataset['train'].map(preprocess_function, remove_columns=\"audio\", batched=False)\n\n# Apply the same preprocess_function to the 'test' split of the dataset, removing the 'audio' column\ndataset['test'] = dataset['test'].map(preprocess_function, remove_columns=\"audio\", batched=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:28:39.389922Z","iopub.execute_input":"2023-12-17T12:28:39.39021Z","iopub.status.idle":"2023-12-17T12:46:45.203667Z","shell.execute_reply.started":"2023-12-17T12:28:39.390185Z","shell.execute_reply":"2023-12-17T12:46:45.202607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:46:45.20621Z","iopub.execute_input":"2023-12-17T12:46:45.2066Z","iopub.status.idle":"2023-12-17T12:46:45.498127Z","shell.execute_reply.started":"2023-12-17T12:46:45.206565Z","shell.execute_reply":"2023-12-17T12:46:45.497244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the \"accuracy\" metric using the evaluate.load() function.\naccuracy = evaluate.load(\"accuracy\")\n\n# Define a function to compute evaluation metrics, which takes eval_pred as input.\ndef compute_metrics(eval_pred):\n    # Extract the model's predictions from eval_pred.\n    predictions = eval_pred.predictions\n    \n    # Apply the softmax function to convert prediction scores into probabilities.\n    predictions = np.exp(predictions) / np.exp(predictions).sum(axis=1, keepdims=True)\n    \n    # Extract the true label IDs from eval_pred.\n    label_ids = eval_pred.label_ids\n    \n    # Calculate accuracy using the loaded accuracy metric by comparing predicted classes\n    # (argmax of probabilities) with the true label IDs.\n    acc_score = accuracy.compute(predictions=predictions.argmax(axis=1), references=label_ids)['accuracy']\n    \n    # Return the computed accuracy as a dictionary with a key \"accuracy.\"\n    return {\n        \"accuracy\": acc_score\n    }","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:46:45.49932Z","iopub.execute_input":"2023-12-17T12:46:45.499665Z","iopub.status.idle":"2023-12-17T12:46:47.100294Z","shell.execute_reply.started":"2023-12-17T12:46:45.499624Z","shell.execute_reply":"2023-12-17T12:46:47.099389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the batch size for training data\nbatch_size = 4\n\n# Define the number of warmup steps for learning rate scheduling\nwarmup_steps = 50\n\n# Define the weight decay for regularization\nweight_decay = 0.02\n\n# Define the number of training epochs\nnum_train_epochs = 10\n\n# Define the name for the model directory\nmodel_name = \"bird_sounds_classification\"\n\n# Create TrainingArguments object to configure the training process\ntraining_args = TrainingArguments(\n    output_dir=model_name,  # Directory to save the model\n    logging_dir='./logs',  # Directory for training logs\n    num_train_epochs=num_train_epochs,  # Number of training epochs\n    per_device_train_batch_size=batch_size,  # Batch size for training\n    per_device_eval_batch_size=batch_size,  # Batch size for evaluation\n    learning_rate=3e-6,  # Learning rate for training\n    logging_strategy='steps',  # Log at specified steps\n    logging_first_step=True,  # Log the first step\n    load_best_model_at_end=True,  # Load the best model at the end of training\n    logging_steps=1,  # Log every step\n    evaluation_strategy='epoch',  # Evaluate at the end of each epoch\n    warmup_steps=warmup_steps,  # Number of warmup steps for learning rate\n    weight_decay=weight_decay,  # Weight decay for regularization\n    eval_steps=1,  # Evaluate every step\n    gradient_accumulation_steps=1,  # Number of gradient accumulation steps\n    gradient_checkpointing=True,  # Enable gradient checkpointing\n    save_strategy='epoch',  # Save model at the end of each epoch\n    save_total_limit=1,  # Limit the number of saved checkpoints\n    report_to=\"mlflow\",  # Log training information to MLflow\n)\n\n# Create a Trainer object to manage the training process\ntrainer = Trainer(\n    model=model,  # The model to be trained\n    args=training_args,  # Training configuration\n    train_dataset=dataset['train'],  # Training dataset\n    eval_dataset=dataset['test'],  # Evaluation dataset\n    tokenizer=feature_extractor,  # Tokenizer\n    compute_metrics=compute_metrics, # Compute metrics\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:46:47.101656Z","iopub.execute_input":"2023-12-17T12:46:47.101975Z","iopub.status.idle":"2023-12-17T12:46:50.921249Z","shell.execute_reply.started":"2023-12-17T12:46:47.101946Z","shell.execute_reply":"2023-12-17T12:46:50.920424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the trained model's performance on a validation or test dataset.\n# This step is crucial for assessing how well the model generalizes to unseen data.\n\n# The `trainer.evaluate()` method typically computes metrics such as accuracy, loss, F1-score,\n# or any other relevant metrics specified during model training.\n\n# It uses the validation or test dataset specified during training to make predictions\n# and then compares these predictions to the actual target values to calculate the metrics.\n\n# The results of this evaluation can provide insights into the model's performance and help\n# identify areas for improvement or optimization.\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:46:50.922355Z","iopub.execute_input":"2023-12-17T12:46:50.922649Z","iopub.status.idle":"2023-12-17T12:49:05.349346Z","shell.execute_reply.started":"2023-12-17T12:46:50.922622Z","shell.execute_reply":"2023-12-17T12:49:05.348439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This line of code initiates the training process for the model using the 'trainer' object.\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T12:49:05.350375Z","iopub.execute_input":"2023-12-17T12:49:05.350622Z","iopub.status.idle":"2023-12-17T19:01:45.146437Z","shell.execute_reply.started":"2023-12-17T12:49:05.3506Z","shell.execute_reply":"2023-12-17T19:01:45.145421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model using the trainer's built-in evaluation function.\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:01:45.147829Z","iopub.execute_input":"2023-12-17T19:01:45.148152Z","iopub.status.idle":"2023-12-17T19:04:04.401032Z","shell.execute_reply.started":"2023-12-17T19:01:45.148124Z","shell.execute_reply":"2023-12-17T19:04:04.400095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the trained 'trainer' to make predictions on the test dataset.\noutputs = trainer.predict(dataset['test'])\n\n# Print the metrics obtained from the prediction outputs.\nprint(outputs.metrics)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:04:04.402535Z","iopub.execute_input":"2023-12-17T19:04:04.402812Z","iopub.status.idle":"2023-12-17T19:06:23.412638Z","shell.execute_reply.started":"2023-12-17T19:04:04.402786Z","shell.execute_reply":"2023-12-17T19:06:23.411632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the true labels from the model outputs\ny_true = outputs.label_ids\n\n# Predict the labels by selecting the class with the highest probability\ny_pred = outputs.predictions.argmax(1)\n\n# Define a function to plot a confusion matrix\ndef plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues, figsize=(10, 8), is_norm=True):\n    \"\"\"\n    This function plots a confusion matrix.\n\n    Parameters:\n        cm (array-like): Confusion matrix as returned by sklearn.metrics.confusion_matrix.\n        classes (list): List of class names, e.g., ['Class 0', 'Class 1'].\n        title (str): Title for the plot.\n        cmap (matplotlib colormap): Colormap for the plot.\n    \"\"\"\n    # Create a figure with a specified size\n    plt.figure(figsize=figsize)\n    \n    \n    # Display the confusion matrix as an image with a colormap\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    # Define tick marks and labels for the classes on the axes\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    \n    if is_norm:\n        fmt = '.3f'\n    else:\n        fmt = '.0f'\n    # Add text annotations to the plot indicating the values in the cells\n    thresh = cm.max() / 2.0\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    # Label the axes\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    # Ensure the plot layout is tight\n    plt.tight_layout()\n    # Display the plot\n    plt.show()\n\n# Calculate accuracy and F1 score\naccuracy = accuracy_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred, average='macro')\n\n# Display accuracy and F1 score\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# Get the confusion matrix if there are a relatively small number of labels\nif len(labels_list) <= 120:\n    # Compute the confusion matrix\n    cm = confusion_matrix(y_true, y_pred) # normalize='true'\n\n    # Plot the confusion matrix using the defined function\n    plot_confusion_matrix(cm, labels_list, figsize=(18, 16), is_norm=False)\n\n# Finally, display classification report\nprint()\nprint(\"Classification report:\")\nprint()\nprint(classification_report(y_true, y_pred, target_names=labels_list, digits=4))","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:06:23.414037Z","iopub.execute_input":"2023-12-17T19:06:23.41485Z","iopub.status.idle":"2023-12-17T19:06:29.226667Z","shell.execute_reply.started":"2023-12-17T19:06:23.414812Z","shell.execute_reply":"2023-12-17T19:06:29.225724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the trained model to a file for future use.\ntrainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:06:29.231408Z","iopub.execute_input":"2023-12-17T19:06:29.231717Z","iopub.status.idle":"2023-12-17T19:06:29.81011Z","shell.execute_reply.started":"2023-12-17T19:06:29.23169Z","shell.execute_reply":"2023-12-17T19:06:29.809106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the device on which you want to run the pipeline (e.g., GPU with device=0)\ndevice = 0  # Change to the appropriate device index if needed\n\n# Create a pipeline for audio classification with the specified model and device\npipe = pipeline('audio-classification', model=model_name, device=device)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:06:29.811283Z","iopub.execute_input":"2023-12-17T19:06:29.811569Z","iopub.status.idle":"2023-12-17T19:06:30.95914Z","shell.execute_reply.started":"2023-12-17T19:06:29.811544Z","shell.execute_reply":"2023-12-17T19:06:30.958111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the audio file of the Andean Guan bird\naudio, rate = torchaudio.load('/kaggle/input/sound-of-114-species-of-birds-till-2022/Voice of Birds/Voice of Birds/Andean Guan_sound/Andean Guan11.mp3')\n\n# Define a resampling transformation to match a specific sample rate (RATE_HZ)\ntransform = torchaudio.transforms.Resample(rate, RATE_HZ)\n\n# Apply the resampling transformation to the audio\naudio = transform(audio).numpy().reshape(-1)\n\n# Create a classification pipeline and analyze the audio to identify the top 10 bird species\n# This assumes 'pipe' is a function or method that performs the classification.\n# If you have the 'pipe' function defined elsewhere, you should ensure it's correctly implemented.\n# You might also want to provide more information about 'pipe' for better context.\npipe(audio, top_k=10)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:06:30.960536Z","iopub.execute_input":"2023-12-17T19:06:30.961313Z","iopub.status.idle":"2023-12-17T19:06:31.11484Z","shell.execute_reply.started":"2023-12-17T19:06:30.961275Z","shell.execute_reply":"2023-12-17T19:06:31.11394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finally, show the audio\nAudio(audio,rate=RATE_HZ)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:06:31.116102Z","iopub.execute_input":"2023-12-17T19:06:31.116396Z","iopub.status.idle":"2023-12-17T19:06:31.142129Z","shell.execute_reply.started":"2023-12-17T19:06:31.116371Z","shell.execute_reply":"2023-12-17T19:06:31.14121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Send model to Huggingface","metadata":{}},{"cell_type":"code","source":"# Import the necessary module to interact with the Hugging Face Hub.\nfrom huggingface_hub import notebook_login\n\n# Perform a login to the Hugging Face Hub.\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:12:44.165732Z","iopub.execute_input":"2023-12-17T19:12:44.166682Z","iopub.status.idle":"2023-12-17T19:12:44.194054Z","shell.execute_reply.started":"2023-12-17T19:12:44.166643Z","shell.execute_reply":"2023-12-17T19:12:44.193203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the HfApi class from the huggingface_hub library.\nfrom huggingface_hub import HfApi\n\n# Create an instance of the HfApi class.\napi = HfApi()\n\n# Define the repository ID by combining the username \"dima806\" with the model name.\nrepo_id = f\"dima806/{model_name}\"\n\ntry:\n    # Attempt to create a new repository on the Hugging Face Model Hub using the specified repo_id.\n    api.create_repo(repo_id)\n    \n    # If the repository creation is successful, print a message indicating that the repository was created.\n    print(f\"Repo {repo_id} created\")\nexcept:\n    # If an exception is raised, print a message indicating that the repository already exists.\n    print(f\"Repo {repo_id} already exists\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:12:57.005793Z","iopub.execute_input":"2023-12-17T19:12:57.006202Z","iopub.status.idle":"2023-12-17T19:12:57.241115Z","shell.execute_reply.started":"2023-12-17T19:12:57.006171Z","shell.execute_reply":"2023-12-17T19:12:57.24003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Uploading a folder to the Hugging Face Model Hub\napi.upload_folder(\n    folder_path=model_name,  # The path to the folder to be uploaded\n    path_in_repo=\".\",  # The path where the folder will be stored in the repository\n    repo_id=repo_id,  # The ID of the repository where the folder will be uploaded\n    repo_type=\"model\",  # The type of the repository (in this case, a model repository)\n    revision=\"main\" # Revision name\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:12:58.487218Z","iopub.execute_input":"2023-12-17T19:12:58.487593Z","iopub.status.idle":"2023-12-17T19:13:37.054066Z","shell.execute_reply.started":"2023-12-17T19:12:58.487562Z","shell.execute_reply":"2023-12-17T19:13:37.053021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}