{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreisMarco/02456_G128_bird_classification/blob/main/scripts/06_Audio_classification_with_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWFh3ZDd2xfs"
      },
      "source": [
        "- check: https://towardsdatascience.com/cnns-for-audio-classification-6244954665ab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9f-FLXXbpSd"
      },
      "source": [
        "# 1. Set up environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LAJw6egX4rhI"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYDdhe_hbtkN"
      },
      "source": [
        "## 1.1 Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8a0qjOlnb-wL"
      },
      "outputs": [],
      "source": [
        "# setting up Drive and path for data loading and saving\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# for data processing\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoFeatureExtractor\n",
        "\n",
        "# for model training and evaluation\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# for visualisation\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0W60ydnPzyuK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94dca4ab-f0d3-447b-af4c-965ccb8a35ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIjtS5eHbv2_"
      },
      "source": [
        "## 1.2 Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HxnBNBYLhCcz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06d4f5b-fc92-4fb7-f379-d61f6987be1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount Drive and set path\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/Deep Learning - DTU 2024/'\n",
        "os.chdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rlmgqFc1fNwM"
      },
      "outputs": [],
      "source": [
        "# load (one batch of) preprocessed data\n",
        "batch_path = 'batch_1'\n",
        "dataset = Dataset.load_from_disk(batch_path).remove_columns('__index_level_0__')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujUq-Thh08fG",
        "outputId": "fcdb2fad-92a5-4209-c111-fa58bce88a9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['audio', 'label'],\n",
              "    num_rows: 1244\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# inspect structure\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMGn4r7zCo8F",
        "outputId": "2859e00e-f6e8-49f7-d406-9b6165e516bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['audio', 'label'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "dataset[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WdTxGAbpAUH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c6cea6c-88b5-49e5-c21c-921b9ae19d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes in the dataset: 4\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(set(dataset[\"label\"]))\n",
        "print(f\"Number of classes in the dataset: {num_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVtW2mpyAcBW"
      },
      "source": [
        "### 1.2.x Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guAsdpSUyoOX",
        "outputId": "2b109735-ab82-4e78-b83c-3151b7f5a370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_dir = 'facebook/wav2vec2-base-960h'\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja3d46_rvDvL",
        "outputId": "87645f98-967c-4cd8-aa52-1de15464355c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed dataset with feature extractor.\n"
          ]
        }
      ],
      "source": [
        "# extract features - from Marco's code\n",
        "def preprocess_function(example):\n",
        "    inputs = feature_extractor(example['audio'], sampling_rate=16000, padding=True)\n",
        "    return inputs\n",
        "dataset = dataset.map(preprocess_function, remove_columns=\"audio\", batched=True, batch_size=32)\n",
        "print(\"Preprocessed dataset with feature extractor.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljR-90TiAtDh"
      },
      "source": [
        "### 1.2.x Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "U3BfD0nTufmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd0682ec-e461-4e2f-eae2-ae16d664bc84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split dataset into training and testing.\n"
          ]
        }
      ],
      "source": [
        "# split dataset into train, val and test - from Marco's code\n",
        "dataset = dataset.train_test_split(test_size=0.1, shuffle=True, stratify_by_column=\"label\", seed=42)\n",
        "print(\"Split dataset into training and testing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY6LLRisAz86"
      },
      "source": [
        "### 1.2.x Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ppAflW2gzTjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e707556-d8fe-4277-d47f-a8a3ee640ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed class weights: {19: 1.5985714285714285, 40: 0.8797169811320755, 47: 1.8048387096774194, 49: 0.5939490445859873}\n",
            "Class weights tensor moved to device: cuda\n"
          ]
        }
      ],
      "source": [
        "# from Marco's code - not sure if needed\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Compute class weights and store in a dict\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(dataset['train']['label']), y=dataset['train']['label'])\n",
        "class_weights = {class_id: weight for class_id, weight in zip(np.unique(dataset['train']['label']), class_weights)}\n",
        "print(f\"Computed class weights: {class_weights}\")\n",
        "# Convert weights to Tensor\n",
        "class_weight_tensor = torch.tensor(list(class_weights.values()), dtype=torch.float32).to(device)\n",
        "print(f\"Class weights tensor moved to device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z62Cf5TA9uP"
      },
      "source": [
        "### 1.2.x Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "R-7sDzNGzuuE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69086ca5-d8bd-4aaa-d5bf-14adc1640f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded evaluation metric: accuracy.\n"
          ]
        }
      ],
      "source": [
        "# from Marco's code\n",
        "\n",
        "import evaluate\n",
        "\n",
        "# Use accuracy as performace metric\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "print(\"Loaded evaluation metric: accuracy.\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    # Extract the model's predictions from eval_pred.\n",
        "    predictions = eval_pred.predictions\n",
        "    # Apply the softmax function to convert prediction scores into probabilities.\n",
        "    predictions = np.exp(predictions) / np.exp(predictions).sum(axis=1, keepdims=True)\n",
        "    # Extract the true label IDs from eval_pred.\n",
        "    label_ids = eval_pred.label_ids\n",
        "    # Calculate accuracy using the loaded accuracy metric by comparing predicted classes\n",
        "    # (argmax of probabilities) with the true label IDs.\n",
        "    acc_score = accuracy.compute(predictions=predictions.argmax(axis=1), references=label_ids)['accuracy']\n",
        "    # Return the computed accuracy as a dictionary with a key \"accuracy.\"\n",
        "    return {\n",
        "        \"accuracy\": acc_score\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMiyPO_BBPL7"
      },
      "source": [
        "### 1.2.x Dataset to Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XQvircD4-voO"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# collate function for padding\n",
        "def collate_fn(batch):\n",
        "    inputs = [torch.tensor(item['input_values']) for item in batch]\n",
        "    labels = [item['label'] for item in batch]\n",
        "    padded_inputs = pad_sequence(inputs, batch_first=True)\n",
        "    return padded_inputs, torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zJakTgTJDGIz"
      },
      "outputs": [],
      "source": [
        "# convert dataset col to tensors\n",
        "dataset['train'].set_format(type='torch', columns=['input_values', 'label'])\n",
        "dataset['test'].set_format(type='torch', columns=['input_values', 'label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vBhPmWWzDHEz"
      },
      "outputs": [],
      "source": [
        "# load data into trainloader\n",
        "train_loader = torch.utils.data.DataLoader(dataset['train'], batch_size=16, collate_fn=collate_fn)\n",
        "test_loader = torch.utils.data.DataLoader(dataset['test'], batch_size=16, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aGpkOMLEFgJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5363fd-8ab2-440b-f186-f1f3da3953ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[ 9.7966e-01,  5.0985e-01, -1.0090e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-3.1648e-01,  7.4693e-01,  1.3755e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 1.5372e-01,  8.3759e-01,  7.3054e-01,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [-8.1476e-01,  4.2780e-01,  7.5354e-01,  ...,  7.9526e-04,\n",
            "          7.9526e-04,  7.9526e-04],\n",
            "        [ 4.9028e-01,  8.0028e-01,  9.4498e-01,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 4.9799e-01,  5.5678e-01, -2.0586e-02,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]]), tensor([19, 40, 40, 19, 49, 49, 49, 40, 47, 47, 49, 47, 49, 40, 49, 49]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-409e81cfdb4c>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  inputs = [torch.tensor(item['input_values']) for item in batch]\n"
          ]
        }
      ],
      "source": [
        "# inspecting data format\n",
        "for batch in train_loader:\n",
        "    print(batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLpS3Q3FBCuW"
      },
      "source": [
        "### 1.2.x Intitiating CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CmWTm254hZjX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class AudioCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(AudioCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool1d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 40000, 256)  # input size should be 160000, which is confirmed wile the model was trained\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))  # First convolution and pooling\n",
        "        print(\"Shape after conv1:\", x.shape)  # Print the shape after conv1\n",
        "        x = self.pool(self.relu(self.conv2(x)))  # Second convolution and pooling\n",
        "        print(\"Shape after conv2:\", x.shape)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        print(\"Shape after flattening:\", x.shape)\n",
        "        x = self.relu(self.fc1(x))  # First fully connected layer\n",
        "        print(\"Shape after fc1:\", x.shape)\n",
        "        # x = self.dropout(x)\n",
        "        # print(\"Shape after dropout:\", x.shape)\n",
        "        x = self.fc2(x)  # Second fully connected layer (output)\n",
        "        print(\"Shape after fc2:\", x.shape)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cL5L5id6b2hH"
      },
      "outputs": [],
      "source": [
        "# # adapted from CIFAR exercise\n",
        "# class CNN_audio(nn.Module):\n",
        "#     def __init__(self, num_classes):\n",
        "#         super().__init__()\n",
        "#         self.num_classes = num_classes\n",
        "#         # Three convolutional layers\n",
        "#         self.conv1 = nn.Conv1d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "#         self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "#         self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "#         # Fully connected layers\n",
        "#         self.fc1 = nn.Linear(4 * 4 * 64, 256)  # Input size calculated dynamically in forward method\n",
        "#         self.fc2 = nn.Linear(256, 128)\n",
        "#         self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "#         # Batch Normalization after each layer for Optimization\n",
        "#         self.bn1 = nn.BatchNorm1d(16)\n",
        "#         self.bn2 = nn.BatchNorm1d(32)\n",
        "#         self.bn3 = nn.BatchNorm1d(64)\n",
        "\n",
        "#         # Pooling\n",
        "#         self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "#         # Activation\n",
        "#         self.relu = nn.ReLU() # delete if you prefere F.relu in the forward part below\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Your code here!\n",
        "\n",
        "#         # without batch normalization\n",
        "#         # x = self.pool(self.relu(self.conv1(x)))  # 32x32 -> 16x16\n",
        "#         # x = self.pool(self.relu(self.conv2(x)))  # 16x16 -> 8x8\n",
        "#         # x = self.pool(self.relu(self.conv3(x)))  # 8x8 -> 4x4\n",
        "\n",
        "#         # with batch normalization\n",
        "#         x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
        "#         print(x.shape)\n",
        "#         x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
        "#         print(x.shape)\n",
        "#         x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
        "#         print(x.shape)\n",
        "\n",
        "\n",
        "#         x = x.view(x.size(0), -1)  # Flatten all dimensions except batch\n",
        "#         x = self.relu(self.fc1(x))  # if you get an error regarding this relu, specify it above as: self.relu1 = nn.ReLU() and so forth.\n",
        "#         x = self.relu(self.fc2(x))\n",
        "#         x = self.fc3(x)\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvodsA0S3NJW",
        "outputId": "b7768e23-2db9-4afa-a23e-d5fbd96a9266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AudioCNN(\n",
            "  (conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=2560000, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=4, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = AudioCNN(num_classes).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SJfLJPyp1Vxa"
      },
      "outputs": [],
      "source": [
        "loss_fct = nn.CrossEntropyLoss(weight=class_weight_tensor)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpzrNpTaBXV2"
      },
      "source": [
        "### 1.2.x Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xPRm-COt2dUl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "outputId": "3ed88aff-5964-4dc8-8bba-892f8fc93f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of inputs before passing to model: torch.Size([16, 1, 160000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-409e81cfdb4c>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  inputs = [torch.tensor(item['input_values']) for item in batch]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after conv1: torch.Size([16, 32, 80000])\n",
            "Shape after conv2: torch.Size([16, 64, 40000])\n",
            "Shape after flattening: torch.Size([16, 2560000])\n",
            "Shape after fc1: torch.Size([16, 256])\n",
            "Shape after fc2: torch.Size([16, 4])\n",
            "Unique values in labels: tensor([19, 40, 47, 49], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-725d7f6236ca>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    218\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                     new_grads.append(\n\u001b[0;32m--> 220\u001b[0;31m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                     )\n\u001b[1;32m    222\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Add the channel dimension for Conv1d: [batch_size, 1, sequence_length]\n",
        "        inputs = inputs.unsqueeze(1)  # Shape: [batch_size, 1, sequence_length]\n",
        "\n",
        "        print(f\"Shape of inputs before passing to model: {inputs.shape}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        print(f\"Unique values in labels: {torch.unique(labels)}\")\n",
        "\n",
        "        labels = labels.long()\n",
        "        loss = loss_fct(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dMYK52j0_n1",
        "outputId": "00775e22-a2ca-4ad0-c6eb-b3dbdaee12a7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0V8PshkCEGa"
      },
      "source": [
        "### 1.2.x Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-tenOFV2sYa"
      },
      "outputs": [],
      "source": [
        "# model.eval()\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# with torch.no_grad():\n",
        "#     for inputs, labels in test_loader:\n",
        "#         inputs, labels = inputs.to(device), labels.to(device)\n",
        "#         outputs = model(inputs)\n",
        "#         _, predicted = torch.max(outputs, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "# accuracy = correct / total\n",
        "# print(f\"Test Accuracy: {accuracy:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}